#!/usr/bin/env python
# coding: utf-8

# ##RBF

# In[13]:


#Library Imports 
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel


# In[14]:


#æ•°æ®å‡†å¤‡  Data Preparation
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å° Sliding Window and test size
N = 6
test_size = 53  # ä½¿ç”¨æœ€å 53 ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡ Feature Construction
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† Train-Test Split
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]


# In[15]:


# å»ºç«‹å¹¶è®­ç»ƒ Gaussian Process æ¨¡å‹

kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3)) #Kernel Definition

#Model Construction and Training
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=0.01,
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)

# 95% ç½®ä¿¡åŒºé—´é¢„æµ‹ Prediction and Evaluation

y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2  = r2_score(y_test, y_pred)
# è®¡ç®—ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
ape = np.abs((y_test - y_pred) / y_test) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
# è®¡ç®—MAPE
mape = np.mean(ape)

# å…ˆéªŒã€åéªŒé‡‡æ · Prior and Posterior Samples

n_prior  = 5   # å…ˆéªŒæ›²çº¿æ•°é‡
n_post   = 5   # åéªŒæ›²çº¿æ•°é‡

#å…ˆéªŒé‡‡æ ·
K_prior = gp.kernel_(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=n_prior
)  # shape (n_prior, n_test)

#åéªŒé‡‡æ ·
posterior_samples = gp.sample_y(
    X_test,
    n_samples=n_post,
    random_state=123
).T  


# In[16]:


#ç»˜å›¾ plotting
plt.figure(figsize=(18, 4))
plt.subplot(1,4,1)
plt.plot(y_test,  'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')

plt.subplot(1,4,2)
for i in range(n_prior):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'Gaussian Process PRIOR  ({n_prior} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,3)
#åéªŒæ ·æœ¬æ›²çº¿
for i in range(n_post):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'Gaussian Process POSTERIOR  ({n_post} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,4)
plt.plot(y_test,  'bo', label='Actual')
plt.plot(y_pred,  'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f"Kernel: {gp.kernel_}")
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()


plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('  Error (%)', fontsize=12)
plt.title('Prediction Error ', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
plt.show()


# In[17]:


# æ‰“å°ç»“æœ Result Output
print(f"\nRMSE = {mse:.2f}")
print(f"\nMAPE = {mape:.2f}")
print(f"\nR2 = {r2:.2f}")

# æ‰“å°ä¼˜åŒ–åçš„æ ¸å‡½æ•°å‚æ•°
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# In[18]:


# Library Imports 
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel

# æ•°æ®å‡†å¤‡ Data Preparation
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å°
N = 6
test_size = 53

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡ Feature Construction
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† Train-Test Split
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]

# ä¸»æ ¸å‡½æ•°ï¼ˆæ¨¡å‹ä½¿ç”¨ï¼‰å¸¦è‡ªåŠ¨ä¼˜åŒ–
kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))          + WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 1e1))

# æ„å»ºé«˜æ–¯è¿‡ç¨‹æ¨¡å‹ï¼ˆå¸¦ä¼˜åŒ–ï¼‰
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=0.0,  # ç”¨ WhiteKernel ä»£æ›¿ alpha
    normalize_y=True,
    n_restarts_optimizer=10,
    random_state=40
)

# æ¨¡å‹è®­ç»ƒ
gp.fit(X_train, y_train)

# æ¨¡å‹é¢„æµ‹
y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2  = r2_score(y_test, y_pred)
ape = np.abs((y_test - y_pred) / y_test) * 100
mape = np.mean(ape)

# ===== âœ… å…ˆéªŒé‡‡æ ·ç”¨æ‰‹åŠ¨æ ¸å‡½æ•° =====
manual_prior_kernel = ConstantKernel(1.0) * RBF(length_scale=5.0)  # å¯å°è¯• 2~10
K_prior = manual_prior_kernel(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=5
)

# åéªŒé‡‡æ ·ï¼ˆä»ä½¿ç”¨æ¨¡å‹ï¼‰
posterior_samples = gp.sample_y(X_test, n_samples=5, random_state=123).T

# ç»˜å›¾
plt.figure(figsize=(18, 4))

# å›¾1ï¼šçœŸå®å€¼
plt.subplot(1, 4, 1)
plt.plot(y_test, 'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')

# å›¾2ï¼šå…ˆéªŒæ ·æœ¬ï¼ˆæ‰‹åŠ¨æ ¸ï¼‰
plt.subplot(1, 4, 2)
for i in range(5):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'Gaussian Process PRIOR (manual kernel)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)

# å›¾3ï¼šåéªŒæ ·æœ¬ï¼ˆæ¨¡å‹æ ¸ï¼‰
plt.subplot(1, 4, 3)
for i in range(5):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'Gaussian Process POSTERIOR (model kernel)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)

# å›¾4ï¼šé¢„æµ‹ + ç½®ä¿¡åŒºé—´
plt.subplot(1, 4, 4)
plt.plot(y_test, 'bo', label='Actual')
plt.plot(y_pred, 'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f"Kernel: {gp.kernel_}")
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()

# å›¾5ï¼šè¯¯å·®å›¾
plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index')
plt.ylabel('Error (%)')
plt.title('Prediction Error')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# æ‰“å°è¯„ä¼°æŒ‡æ ‡
print(f"\nRMSE = {mse:.2f}")
print(f"MAPE = {mape:.2f}")
print(f"RÂ²    = {r2:.2f}")

# è¾“å‡ºä¼˜åŒ–åæ ¸ç»“æ„
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# ##Matern

# In[101]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, ConstantKernel


# In[102]:


#æ•°æ®å‡†å¤‡
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å°
N = 6
test_size = 53  # ä½¿ç”¨æœ€å 53 ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]


# In[103]:


# å»ºç«‹å¹¶è®­ç»ƒ Gaussian Process æ¨¡å‹

kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=0.01,
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)

# 95% ç½®ä¿¡åŒºé—´é¢„æµ‹

y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2  = r2_score(y_test, y_pred)
# è®¡ç®—ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
ape = np.abs((y_test - y_pred) / y_test) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
# è®¡ç®—MAPE
mape = np.mean(ape)

# å…ˆéªŒã€åéªŒé‡‡æ ·

n_prior  = 5   # å…ˆéªŒæ›²çº¿æ•°é‡
n_post   = 5   # åéªŒæ›²çº¿æ•°é‡

#å…ˆéªŒé‡‡æ ·
K_prior = gp.kernel_(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=n_prior
)  # shape (n_prior, n_test)

#åéªŒé‡‡æ ·
posterior_samples = gp.sample_y(
    X_test,
    n_samples=n_post,
    random_state=123
).T  


# In[104]:


#ç»˜å›¾
plt.figure(figsize=(18, 4))
plt.subplot(1,4,1)
plt.plot(y_test,  'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')

plt.subplot(1,4,2)
for i in range(n_prior):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'Gaussian Process PRIOR  ({n_prior} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,3)
#åéªŒæ ·æœ¬æ›²çº¿
for i in range(n_post):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'Gaussian Process POSTERIOR  ({n_post} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,4)
plt.plot(y_test,  'bo', label='Actual')
plt.plot(y_pred,  'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f"Kernel: {gp.kernel_}")
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()


plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('  Error (%)', fontsize=12)
plt.title('Prediction Error ', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
plt.show()


# In[105]:


# æ‰“å°ç»“æœ
print(f"\nRMSE = {mse:.2f}")
print(f"\nMAPE = {mape:.2f}")
print(f"\nR2 = {r2:.2f}")

# æ‰“å°ä¼˜åŒ–åçš„æ ¸å‡½æ•°å‚æ•°
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# ##RQ

# In[106]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RationalQuadratic, ConstantKernel


# In[107]:


#æ•°æ®å‡†å¤‡
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å°
N = 6
test_size = 53  # ä½¿ç”¨æœ€å 53 ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]


# å»ºç«‹å¹¶è®­ç»ƒ Gaussian Process æ¨¡å‹

kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=0.01,
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)


# 95% ç½®ä¿¡åŒºé—´é¢„æµ‹

y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2  = r2_score(y_test, y_pred)
# è®¡ç®—ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
ape = np.abs((y_test - y_pred) / y_test) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
# è®¡ç®—MAPE
mape = np.mean(ape)

# å…ˆéªŒã€åéªŒé‡‡æ ·

n_prior  = 5   # å…ˆéªŒæ›²çº¿æ•°é‡
n_post   = 5   # åéªŒæ›²çº¿æ•°é‡

#å…ˆéªŒé‡‡æ ·
K_prior = gp.kernel_(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=n_prior
)  # shape (n_prior, n_test)

#åéªŒé‡‡æ ·
posterior_samples = gp.sample_y(
    X_test,
    n_samples=n_post,
    random_state=123
).T  


# In[108]:


#ç»˜å›¾
plt.figure(figsize=(18, 4))
plt.subplot(1,4,1)
plt.plot(y_test,  'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')

plt.subplot(1,4,2)
for i in range(n_prior):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'Gaussian Process PRIOR  ({n_prior} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,3)
#åéªŒæ ·æœ¬æ›²çº¿
for i in range(n_post):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'Gaussian Process POSTERIOR  ({n_post} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1,4,4)
plt.plot(y_test,  'bo', label='Actual')
plt.plot(y_pred,  'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f"Kernel: {gp.kernel_}")
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()


plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('  Error (%)', fontsize=12)
plt.title('Prediction Error ', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
plt.show()


# In[109]:


# æ‰“å°ç»“æœ
print(f"\nRMSE = {mse:.2f}")
print(f"\nMAPE = {mape:.2f}")
print(f"\nR2 = {r2:.2f}")

# æ‰“å°ä¼˜åŒ–åçš„æ ¸å‡½æ•°å‚æ•°
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# In[11]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ExpSineSquared, ConstantKernel

# æ•°æ®å‡†å¤‡
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')  # æ›¿æ¢ä¸ºä½ çš„è·¯å¾„
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å°
N = 6
test_size = 53

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]

# åˆ›å»ºæ”¹è¿›åçš„å‘¨æœŸæ ¸å‡½æ•°ï¼ˆæ›´åˆç†çš„åˆå§‹å€¼ + è¾¹ç•Œï¼‰
kernel = ConstantKernel(1.0, (1e-2, 1e2)) * ExpSineSquared(
    length_scale=2.0, periodicity=26.0,  # åŠå¹´ä¸ºå‘¨æœŸ
    length_scale_bounds=(1e-1, 1e2),
    periodicity_bounds=(5.0, 100.0)
)

# æ„å»º GPR æ¨¡å‹å¹¶æ‹Ÿåˆ
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=1e-2,  # å¢å¤§ alpha æå‡æ•°å€¼ç¨³å®šæ€§
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)

# é¢„æµ‹åŠç½®ä¿¡åŒºé—´
y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2  = r2_score(y_test, y_pred)
ape = np.abs((y_test - y_pred) / y_test) * 100
mape = np.mean(ape)

# å…ˆéªŒé‡‡æ ·
n_prior = 5
K_prior = gp.kernel_(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=n_prior
)

# åéªŒé‡‡æ ·
n_post = 5
posterior_samples = gp.sample_y(X_test, n_samples=n_post, random_state=123).T

# å›¾1ï¼šçœŸå® vs é¢„æµ‹
plt.figure(figsize=(18, 4))
plt.subplot(1, 4, 1)
plt.plot(y_test,  'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Pig Price')

# å›¾2ï¼šå…ˆéªŒæ ·æœ¬
plt.subplot(1, 4, 2)
for i in range(n_prior):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'GP PRIOR  ({n_prior} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)

# å›¾3ï¼šåéªŒæ ·æœ¬
plt.subplot(1, 4, 3)
for i in range(n_post):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'GP POSTERIOR  ({n_post} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)

# å›¾4ï¼šé¢„æµ‹+ç½®ä¿¡åŒºé—´
plt.subplot(1, 4, 4)
plt.plot(y_test,  'bo', label='Actual')
plt.plot(y_pred,  'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f"Kernel: {gp.kernel_}")
plt.xlabel('Test Sample Index')
plt.ylabel('Pig Price')
plt.legend()
plt.grid(True)
plt.tight_layout()

# å›¾5ï¼šè¯¯å·®å›¾
plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('Error (%)', fontsize=12)
plt.title('Prediction Error', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# æ‰“å°è¯„ä¼°æŒ‡æ ‡
print(f"\nRMSE = {mse:.2f}")
print(f"MAPE = {mape:.2f}")
print(f"R2 = {r2:.2f}")

# æ‰“å°ä¼˜åŒ–åçš„æ ¸å‡½æ•°ç»“æ„
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# In[ ]:





# ##RBF + (RQ Ã— LIN)

# In[7]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, RationalQuadratic, DotProduct, ConstantKernel

#æ•°æ®å‡†å¤‡
np.random.seed(40)
df = pd.read_csv(r'E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
data = df['price'].to_numpy()

# è®¾ç½®æ»‘åŠ¨çª—å£é•¿åº¦å’Œæµ‹è¯•é›†å¤§å°
N = 6
test_size = 53  # ä½¿ç”¨æœ€å 53 ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†

# æ„é€ æ»‘åŠ¨çª—å£ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, y_train = X[:-test_size], y[:-test_size]
X_test,  y_test  = X[-test_size:], y[-test_size:]
# æ ¸å‡½æ•°è®¾å®šï¼šRBF + (RQ Ã— LIN)
kernel = ConstantKernel(1.0, (1e-3, 1e3)) * (
    RBF(length_scale=1.0) +
    RationalQuadratic(length_scale=1.0, alpha=1.0) * DotProduct()
)

# æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=1e-6,
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred, y_std = gp.predict(X_test, return_std=True)
conf95 = 1.96 * y_std
mse = np.mean((y_pred - y_test) ** 2)
r2 = r2_score(y_test, y_pred)
ape = np.abs((y_test - y_pred) / y_test) * 100
mape = np.mean(ape)

# å…ˆéªŒä¸åéªŒé‡‡æ ·
n_prior = 5
n_post = 5

K_prior = gp.kernel_(X_test)
prior_samples = np.random.multivariate_normal(
    mean=np.zeros(len(X_test)),
    cov=K_prior + 1e-6 * np.eye(len(X_test)),
    size=n_prior
)

posterior_samples = gp.sample_y(X_test, n_samples=n_post, random_state=123).T

# å¯è§†åŒ–
plt.figure(figsize=(18, 4))
plt.subplot(1, 4, 1)
plt.plot(y_test, 'bo', label='Actual')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')

plt.subplot(1, 4, 2)
for i in range(n_prior):
    plt.plot(prior_samples[i], lw=1.2, label=f'Prior sample {i+1}')
plt.title(f'GP PRIOR Samples ({n_prior} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1, 4, 3)
for i in range(n_post):
    plt.plot(posterior_samples[i], lw=1.2, label=f'Posterior sample {i+1}')
plt.title(f'GP POSTERIOR Samples ({n_post} draws)')
plt.xlabel('Test Sample Index')
plt.ylabel('Sampled Value')
plt.legend(loc='upper right', ncol=3, fontsize='small')
plt.grid(True)
plt.tight_layout()

plt.subplot(1, 4, 4)
plt.plot(y_test, 'bo', label='Actual')
plt.plot(y_pred, 'r-', label='Predicted')
plt.fill_between(
    np.arange(len(y_pred)),
    y_pred - conf95,
    y_pred + conf95,
    color='pink', alpha=0.3, label='95% CI'
)
plt.title(f'GPâ€‘RBF + (RQÃ—LIN) | MSE={mse:.2f}, RÂ²={r2:.2f}')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()

plt.figure(figsize=(6, 4))
plt.plot(range(len(ape)), ((y_test - y_pred) / y_test), color='orange', alpha=0.7)
plt.xlabel('Sample Index', fontsize=12)
plt.ylabel('Error (%)', fontsize=12)
plt.title('Prediction Error', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# æ‰“å°ç»“æœ
print(f"\nRMSE = {mse:.2f}")
print(f"\nMAPE = {mape:.2f}")
print(f"\nR2 = {r2:.2f}")

# æ‰“å°ä¼˜åŒ–åçš„æ ¸å‡½æ•°å‚æ•°
print("\nOptimized kernel:", gp.kernel_)


# In[ ]:





# In[ ]:





# In[ ]:





# In[2]:


import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# è¯»å–æ•°æ®
df = pd.read_csv('E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
ts = df['price']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå53ä¸ªä¸ºæµ‹è¯•é›†ï¼‰
train = ts[:-53]
test = ts[-53:]

# æ‹Ÿåˆ SARIMA æ¨¡å‹
model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 0, 1, 52))
result = model.fit(disp=False)

# é¢„æµ‹åŠç½®ä¿¡åŒºé—´ï¼ˆ95%ï¼‰
forecast_result = result.get_forecast(steps=53)
forecast_mean = forecast_result.predicted_mean
conf_int = forecast_result.conf_int(alpha=0.05)

# å¯è§†åŒ–
plt.figure(figsize=(12, 5))
plt.plot(train, label='Training Set')
plt.plot(test, label='Actual Test', color='orange')
plt.plot(forecast_mean, label='Forecast', color='green')
plt.fill_between(conf_int.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1],
                 color='green', alpha=0.3, label='95% Confidence Interval')
plt.title('Egg Price Forecast with SARIMA and 95% CI')
plt.xlabel('Date')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# è¯„ä¼°æŒ‡æ ‡ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
rmse = mean_squared_error(test, forecast_mean, squared=False)
mae = mean_absolute_error(test, forecast_mean)
r2 = r2_score(test, forecast_mean)

print(f"RMSE = {rmse:.2f}")
print(f"MAE  = {mae:.2f}")
print(f"RÂ²   = {r2:.2f}")


# In[ ]:





# In[9]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, RationalQuadratic, DotProduct, ConstantKernel
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ========== æ•°æ®å‡†å¤‡ ==========
df = pd.read_csv('E:/ç³»ç»Ÿé»˜è®¤/æ¡Œé¢/eggprice.csv')
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
price_series = df['price']
data = price_series.to_numpy()

# ========== SARIMA æ¨¡å‹ ==========
train_ts = price_series[:-53]
test_ts = price_series[-53:]

sarima_model = SARIMAX(train_ts, order=(1, 1, 1), seasonal_order=(1, 0, 1, 52))
sarima_result = sarima_model.fit(disp=False)
sarima_forecast = sarima_result.get_forecast(steps=53)
sarima_pred = sarima_forecast.predicted_mean
sarima_ci = sarima_forecast.conf_int(alpha=0.05)

# ========== GPR æ¨¡å‹ ==========
N = 6
X = np.zeros((len(data) - N, N))
y = np.zeros(len(data) - N)
for i in range(len(data) - N):
    X[i, :] = data[i : i + N]
    y[i]    = data[i + N]

# æ»‘åŠ¨çª—å£åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•
X_train, y_train = X[:-53], y[:-53]
X_test,  y_test  = X[-53:], y[-53:]

kernel = ConstantKernel(1.0, (1e-3, 1e3)) * (
    RBF(length_scale=1.0) +
    RationalQuadratic(length_scale=1.0, alpha=1.0) * DotProduct()
)

gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=1e-6,
    n_restarts_optimizer=10,
    normalize_y=True,
    random_state=40
)
gp.fit(X_train, y_train)
y_pred_gpr, y_std_gpr = gp.predict(X_test, return_std=True)
conf95_gpr = 1.96 * y_std_gpr

# ========== è¯„ä¼°æŒ‡æ ‡ ==========
# SARIMA
sarima_rmse = mean_squared_error(test_ts, sarima_pred, squared=False)
sarima_mae = mean_absolute_error(test_ts, sarima_pred)
sarima_r2 = r2_score(test_ts, sarima_pred)
sarima_mape = np.mean(np.abs((test_ts - sarima_pred) / test_ts)) * 100

# GPR
gpr_rmse = mean_squared_error(y_test, y_pred_gpr, squared=False)
gpr_mae = mean_absolute_error(y_test, y_pred_gpr)
gpr_r2 = r2_score(y_test, y_pred_gpr)
gpr_mape = np.mean(np.abs((y_test - y_pred_gpr) / y_test)) * 100

# ========== å¯è§†åŒ– ==========
plt.figure(figsize=(12, 5))
plt.plot(range(len(y_test)), y_test, 'ko', label='Actual Price')

# SARIMAï¼ˆæ³¨æ„ï¼šè¿™é‡Œæ˜¯è¿ç»­æ—¶é—´é¢„æµ‹ï¼‰
plt.plot(range(len(sarima_pred)), sarima_pred.values, 'g-', label='SARIMA Prediction')
plt.fill_between(range(len(sarima_pred)),
                 sarima_ci.iloc[:, 0],
                 sarima_ci.iloc[:, 1],
                 color='green', alpha=0.2, label='SARIMA 95% CI')

# GPR
plt.plot(range(len(y_pred_gpr)), y_pred_gpr, 'r-', label='GPR Prediction')
plt.fill_between(range(len(y_pred_gpr)),
                 y_pred_gpr - conf95_gpr,
                 y_pred_gpr + conf95_gpr,
                 color='red', alpha=0.2, label='GPR 95% CI')

plt.title('Comparison of GPR vs SARIMA Predictions')
plt.xlabel('Test Sample Index')
plt.ylabel('Egg Price')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ========== è¾“å‡ºè¯„ä¼°æŒ‡æ ‡ ==========
print("ğŸ“Š GPR Performance:")
print(f"  RMSE = {gpr_rmse:.2f}")
print(f"  MAE  = {gpr_mae:.2f}")
print(f"  RÂ²   = {gpr_r2:.2f}")
print(f"  MAPE = {gpr_mape:.2f}%")

print("\nğŸ“Š SARIMA Performance:")
print(f"  RMSE = {sarima_rmse:.2f}")
print(f"  MAE  = {sarima_mae:.2f}")
print(f"  RÂ²   = {sarima_r2:.2f}")
print(f"  MAPE = {sarima_mape:.2f}%")


# In[ ]:





# In[ ]:





# In[ ]:




